{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27807,"status":"ok","timestamp":1759263420734,"user":{"displayName":"prasuk jain","userId":"07084804988335330272"},"user_tz":-330},"id":"AIkIDB1B_5hk","outputId":"77f981f9-0800-4638-da29-22f125678675"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1568,"status":"ok","timestamp":1759263422298,"user":{"displayName":"prasuk jain","userId":"07084804988335330272"},"user_tz":-330},"id":"OJQn4g_mQ1Fb","outputId":"eda44bf6-0aa4-4b87-91c1-2f6d9e7f55ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/DFPIR_project/DFPIR-main\n"]}],"source":["cd drive/MyDrive/DFPIR_project/DFPIR-main/"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1199,"status":"ok","timestamp":1759263423498,"user":{"displayName":"prasuk jain","userId":"07084804988335330272"},"user_tz":-330},"id":"jiNgGo3zr9Or","outputId":"276641c7-e3df-4cf8-a233-af10e445a02b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting requirements.txt\n"]}],"source":["%%writefile requirements.txt\n","torch\u003e=1.8.0\n","torchvision\n","openai-clip #\n","numpy #\n","scikit-image #\n","scikit-video #\n","scipy #\n","matplotlib #\n","einops #\n","huggingface-hub #\n","tqdm #\n","tensorboard #"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10769,"status":"ok","timestamp":1759263434281,"user":{"displayName":"prasuk jain","userId":"07084804988335330272"},"user_tz":-330},"id":"sTsWf-4Kr9Ou","outputId":"9db38bc5-a110-4582-f217-505ef89df1f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch\u003e=1.8.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.8.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (0.23.0+cu126)\n","Collecting openai-clip (from -r requirements.txt (line 3))\n","  Downloading openai-clip-1.0.1.tar.gz (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.0.2)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (0.25.2)\n","Collecting scikit-video (from -r requirements.txt (line 6))\n","  Downloading scikit_video-1.1.11-py2.py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (1.16.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (3.10.0)\n","Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (0.8.1)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (0.35.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (4.67.1)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (2.19.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003e-r requirements.txt (line 1)) (3.19.1)\n","Requirement already satisfied: typing-extensions\u003e=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003e-r requirements.txt (line 1)) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003e-r requirements.txt (line 1)) (75.2.0)\n","Requirement already satisfied: sympy\u003e=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003e-r requirements.txt (line 1)) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003e-r requirements.txt (line 1)) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003e-r requirements.txt (line 1)) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003e-r requirements.txt (line 1)) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003e-r requirements.txt (line 1)) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003e-r requirements.txt (line 1)) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003e-r requirements.txt (line 1)) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003e-r requirements.txt (line 1)) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003e-r requirements.txt (line 1)) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003e-r requirements.txt (line 1)) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003e-r requirements.txt (line 1)) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003e-r requirements.txt (line 1)) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003e-r requirements.txt (line 1)) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003e-r requirements.txt (line 1)) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003e-r requirements.txt (line 1)) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003e-r requirements.txt (line 1)) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003e-r requirements.txt (line 1)) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003e-r requirements.txt (line 1)) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003e-r requirements.txt (line 1)) (3.4.0)\n","Requirement already satisfied: pillow!=8.3.*,\u003e=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision-\u003e-r requirements.txt (line 2)) (11.3.0)\n","Collecting ftfy (from openai-clip-\u003e-r requirements.txt (line 3))\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from openai-clip-\u003e-r requirements.txt (line 3)) (2024.11.6)\n","Requirement already satisfied: imageio!=2.35.0,\u003e=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image-\u003e-r requirements.txt (line 5)) (2.37.0)\n","Requirement already satisfied: tifffile\u003e=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image-\u003e-r requirements.txt (line 5)) (2025.9.9)\n","Requirement already satisfied: packaging\u003e=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image-\u003e-r requirements.txt (line 5)) (25.0)\n","Requirement already satisfied: lazy-loader\u003e=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image-\u003e-r requirements.txt (line 5)) (0.4)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib-\u003e-r requirements.txt (line 8)) (1.3.3)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib-\u003e-r requirements.txt (line 8)) (0.12.1)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib-\u003e-r requirements.txt (line 8)) (4.60.0)\n","Requirement already satisfied: kiwisolver\u003e=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib-\u003e-r requirements.txt (line 8)) (1.4.9)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib-\u003e-r requirements.txt (line 8)) (3.2.4)\n","Requirement already satisfied: python-dateutil\u003e=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib-\u003e-r requirements.txt (line 8)) (2.9.0.post0)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub-\u003e-r requirements.txt (line 10)) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub-\u003e-r requirements.txt (line 10)) (2.32.4)\n","Requirement already satisfied: hf-xet\u003c2.0.0,\u003e=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub-\u003e-r requirements.txt (line 10)) (1.1.10)\n","Requirement already satisfied: absl-py\u003e=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard-\u003e-r requirements.txt (line 12)) (1.4.0)\n","Requirement already satisfied: grpcio\u003e=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard-\u003e-r requirements.txt (line 12)) (1.75.0)\n","Requirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard-\u003e-r requirements.txt (line 12)) (3.9)\n","Requirement already satisfied: protobuf!=4.24.0,\u003e=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard-\u003e-r requirements.txt (line 12)) (5.29.5)\n","Requirement already satisfied: six\u003e1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard-\u003e-r requirements.txt (line 12)) (1.17.0)\n","Requirement already satisfied: tensorboard-data-server\u003c0.8.0,\u003e=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard-\u003e-r requirements.txt (line 12)) (0.7.2)\n","Requirement already satisfied: werkzeug\u003e=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard-\u003e-r requirements.txt (line 12)) (3.1.3)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy\u003e=1.13.3-\u003etorch\u003e=1.8.0-\u003e-r requirements.txt (line 1)) (1.3.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug\u003e=1.0.1-\u003etensorboard-\u003e-r requirements.txt (line 12)) (3.0.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy-\u003eopenai-clip-\u003e-r requirements.txt (line 3)) (0.2.13)\n","Requirement already satisfied: charset_normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.12/dist-packages (from requests-\u003ehuggingface-hub-\u003e-r requirements.txt (line 10)) (3.4.3)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.12/dist-packages (from requests-\u003ehuggingface-hub-\u003e-r requirements.txt (line 10)) (3.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests-\u003ehuggingface-hub-\u003e-r requirements.txt (line 10)) (2.5.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests-\u003ehuggingface-hub-\u003e-r requirements.txt (line 10)) (2025.8.3)\n","Downloading scikit_video-1.1.11-py2.py3-none-any.whl (2.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: openai-clip\n","  Building wheel for openai-clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368605 sha256=1980d0b120ec45f9fedd2a4708d44bff982a5041ae5f84d8eb816ac1fafc742f\n","  Stored in directory: /root/.cache/pip/wheels/ab/49/bc/c2342e8e14878210ba4825cf314a53f2570f6fb18b91fce3cf\n","Successfully built openai-clip\n","Installing collected packages: ftfy, scikit-video, openai-clip\n","Successfully installed ftfy-6.3.1 openai-clip-1.0.1 scikit-video-1.1.11\n"]}],"source":["!pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"4S_fdVYVRcCR"},"source":["#Modify"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49,"status":"ok","timestamp":1759264217477,"user":{"displayName":"prasuk jain","userId":"07084804988335330272"},"user_tz":-330},"id":"D7eDcY_sRfhv","outputId":"61dc9613-f563-4bc8-e82d-e74ed300c22e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting net/model.py\n"]}],"source":["%%writefile net/model.py\n","## PromptIR: Prompting for All-in-One Blind Image Restoration\n","## Vaishnav Potlapalli, Syed Waqas Zamir, Salman Khan, and Fahad Shahbaz Khan\n","## https://arxiv.org/abs/2306.13090\n","\n","\n","import torch, torchvision\n","# print(torch.__version__)\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from pdb import set_trace as stx\n","import numbers\n","\n","from einops import rearrange\n","from einops.layers.torch import Rearrange\n","import time\n","\n","import clip\n","import os\n","import sys\n","from net.arch_util import LayerNorm2d\n","from net.local_arch import Local_Base\n","import torchvision.transforms as transforms\n","from huggingface_hub import PyTorchModelHubMixin\n","\n","##########################################################################\n","## Layer Norm\n","\n","def to_3d(x):\n","    return rearrange(x, 'b c h w -\u003e b (h w) c')\n","\n","def to_4d(x,h,w):\n","    return rearrange(x, 'b (h w) c -\u003e b c h w',h=h,w=w)\n","\n","class BiasFree_LayerNorm(nn.Module):\n","    def __init__(self, normalized_shape):\n","        super(BiasFree_LayerNorm, self).__init__()\n","        if isinstance(normalized_shape, numbers.Integral):\n","            normalized_shape = (normalized_shape,)\n","        normalized_shape = torch.Size(normalized_shape)\n","\n","        assert len(normalized_shape) == 1\n","\n","        self.weight = nn.Parameter(torch.ones(normalized_shape))\n","        self.normalized_shape = normalized_shape\n","\n","    def forward(self, x):\n","        sigma = x.var(-1, keepdim=True, unbiased=False)\n","        return x / torch.sqrt(sigma+1e-5) * self.weight\n","\n","class WithBias_LayerNorm(nn.Module):\n","    def __init__(self, normalized_shape):\n","        super(WithBias_LayerNorm, self).__init__()\n","        if isinstance(normalized_shape, numbers.Integral):\n","            normalized_shape = (normalized_shape,)\n","        normalized_shape = torch.Size(normalized_shape)\n","\n","        assert len(normalized_shape) == 1\n","\n","        self.weight = nn.Parameter(torch.ones(normalized_shape))\n","        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n","        self.normalized_shape = normalized_shape\n","\n","    def forward(self, x):\n","        mu = x.mean(-1, keepdim=True)\n","        sigma = x.var(-1, keepdim=True, unbiased=False)\n","        return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias\n","\n","class LayerNorm(nn.Module):\n","    def __init__(self, dim, LayerNorm_type):\n","        super(LayerNorm, self).__init__()\n","        if LayerNorm_type =='BiasFree':\n","            self.body = BiasFree_LayerNorm(dim)\n","        else:\n","            self.body = WithBias_LayerNorm(dim)\n","\n","    def forward(self, x):\n","        h, w = x.shape[-2:]\n","        return to_4d(self.body(to_3d(x)), h, w)\n","\n","##########################################################################\n","## Gated-Dconv Feed-Forward Network (GDFN)\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, ffn_expansion_factor, bias):\n","        super(FeedForward, self).__init__()\n","\n","        hidden_features = int(dim*ffn_expansion_factor)\n","\n","        self.project_in = nn.Conv2d(dim, hidden_features*2, kernel_size=1, bias=bias)\n","\n","        self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=bias)\n","\n","        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n","\n","    def forward(self, x):\n","        x = self.project_in(x)\n","        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n","        x = F.gelu(x1) * x2\n","        x = self.project_out(x)\n","        return x\n","\n","##########################################################################\n","## Multi-DConv Head Transposed Self-Attention (MDTA)\n","class Attention(nn.Module):\n","    def __init__(self, dim, num_heads, bias):\n","        super(Attention, self).__init__()\n","        self.num_heads = num_heads\n","        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n","\n","        self.qkv = nn.Conv2d(dim, dim*3, kernel_size=1, bias=bias)\n","        self.qkv_dwconv = nn.Conv2d(dim*3, dim*3, kernel_size=3, stride=1, padding=1, groups=dim*3, bias=bias)\n","        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n","    def forward(self, x):\n","        b,c,h,w = x.shape\n","        qkv = self.qkv_dwconv(self.qkv(x))\n","        q,k,v = qkv.chunk(3, dim=1)\n","        q = rearrange(q, 'b (head c) h w -\u003e b head c (h w)', head=self.num_heads)\n","        k = rearrange(k, 'b (head c) h w -\u003e b head c (h w)', head=self.num_heads)\n","        v = rearrange(v, 'b (head c) h w -\u003e b head c (h w)', head=self.num_heads)\n","        q = torch.nn.functional.normalize(q, dim=-1)\n","        k = torch.nn.functional.normalize(k, dim=-1)\n","        attn = (q @ k.transpose(-2, -1)) * self.temperature\n","        attn = attn.softmax(dim=-1)\n","        out = (attn @ v)\n","        out = rearrange(out, 'b head c (h w) -\u003e b (head c) h w', head=self.num_heads, h=h, w=w)\n","\n","        out = self.project_out(out)\n","        return out\n","\n","class resblock(nn.Module):\n","    def __init__(self, dim):\n","\n","        super(resblock, self).__init__()\n","        # self.norm = LayerNorm(dim, LayerNorm_type='BiasFree')\n","\n","        self.body = nn.Sequential(nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, bias=False),\n","                                  nn.PReLU(),\n","                                  nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, bias=False))\n","    def forward(self, x):\n","        res = self.body((x))\n","        res += x\n","        return res\n","\n","\n","##########################################################################\n","## Resizing modules\n","class Downsample(nn.Module):\n","    def __init__(self, n_feat):\n","        super(Downsample, self).__init__()\n","\n","        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat//2, kernel_size=3, stride=1, padding=1, bias=False),\n","                                  nn.PixelUnshuffle(2))\n","    def forward(self, x):\n","        return self.body(x)\n","\n","class Upsample(nn.Module):\n","    def __init__(self, n_feat):\n","        super(Upsample, self).__init__()\n","\n","        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat*2, kernel_size=3, stride=1, padding=1, bias=False),\n","                                  nn.PixelShuffle(2))\n","    def forward(self, x):\n","        return self.body(x)\n","\n","##########################################################################\n","## Transformer Block\n","class TransformerBlock(nn.Module):\n","    def __init__(self, dim, num_heads, ffn_expansion_factor, bias, LayerNorm_type):\n","        super(TransformerBlock, self).__init__()\n","        self.norm1 = LayerNorm(dim, LayerNorm_type)\n","        self.attn = Attention(dim, num_heads, bias)\n","        self.norm2 = LayerNorm(dim, LayerNorm_type)\n","        self.ffn = FeedForward(dim, ffn_expansion_factor, bias)\n","\n","    def forward(self, x):\n","        x = x + self.attn(self.norm1(x))\n","        x = x + self.ffn(self.norm2(x))\n","        return x\n","\n","##########################################################################\n","## Overlapped image patch embedding with 3x3 Conv\n","class OverlapPatchEmbed(nn.Module):\n","    def __init__(self, in_c=3, embed_dim=48, bias=False):\n","        super(OverlapPatchEmbed, self).__init__()\n","        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=3, stride=1, padding=1, bias=bias)\n","    def forward(self, x):\n","        x = self.proj(x)\n","        return x\n","\n","##########################################################################\n","##---------- Prompt Gen Module -----------------------\n","class PromptGenBlock(nn.Module):\n","    def __init__(self,prompt_dim=128,prompt_len=5,prompt_size = 96,lin_dim = 192):\n","        super(PromptGenBlock,self).__init__()\n","        self.prompt_param = nn.Parameter(torch.rand(1,prompt_len,prompt_dim,prompt_size,prompt_size))\n","        self.linear_layer = nn.Linear(lin_dim,prompt_len)\n","        self.conv3x3 = nn.Conv2d(prompt_dim,prompt_dim,kernel_size=3,stride=1,padding=1,bias=False)\n","    def forward(self,x):\n","        B,C,H,W = x.shape\n","        emb = x.mean(dim=(-2,-1))\n","        prompt_weights = F.softmax(self.linear_layer(emb),dim=1)\n","        prompt = prompt_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * self.prompt_param.unsqueeze(0).repeat(B,1,1,1,1,1).squeeze(1)\n","        prompt = torch.sum(prompt,dim=1)\n","        prompt = F.interpolate(prompt,(H,W),mode=\"bilinear\")\n","        prompt = self.conv3x3(prompt)\n","        return prompt\n","\n","\n","\n","\n","\n","##########################################################################\n","##---------- PromptIR -----------------------\n","\n","class PromptIR(nn.Module):\n","    def __init__(self,\n","        inp_channels=3,\n","        out_channels=3,\n","        dim = 48,\n","        num_blocks = [4,6,6,8],\n","        num_refinement_blocks = 4,\n","        heads = [1,2,4,8],\n","        ffn_expansion_factor = 2.66,\n","        bias = False,\n","        LayerNorm_type = 'WithBias',   ## Other option 'BiasFree'\n","        decoder = False,\n","    ):\n","        super(PromptIR, self).__init__()\n","        self.patch_embed = OverlapPatchEmbed(inp_channels, dim)\n","        self.decoder = decoder\n","        if self.decoder:\n","            self.prompt1 = PromptGenBlock(prompt_dim=64,prompt_len=5,prompt_size = 64,lin_dim = 96)\n","            self.prompt2 = PromptGenBlock(prompt_dim=128,prompt_len=5,prompt_size = 32,lin_dim = 192)\n","            self.prompt3 = PromptGenBlock(prompt_dim=320,prompt_len=5,prompt_size = 16,lin_dim = 384)\n","        self.chnl_reduce1 = nn.Conv2d(64,64,kernel_size=1,bias=bias)\n","        self.chnl_reduce2 = nn.Conv2d(128,128,kernel_size=1,bias=bias)\n","        self.chnl_reduce3 = nn.Conv2d(320,256,kernel_size=1,bias=bias)\n","\n","        self.reduce_noise_channel_1 = nn.Conv2d(dim + 64,dim,kernel_size=1,bias=bias)\n","        self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n","\n","        self.down1_2 = Downsample(dim) ## From Level 1 to Level 2\n","\n","        self.reduce_noise_channel_2 = nn.Conv2d(int(dim*2**1) + 128,int(dim*2**1),kernel_size=1,bias=bias)\n","        self.encoder_level2 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n","\n","        self.down2_3 = Downsample(int(dim*2**1)) ## From Level 2 to Level 3\n","\n","        self.reduce_noise_channel_3 = nn.Conv2d(int(dim*2**2) + 256,int(dim*2**2),kernel_size=1,bias=bias)\n","        self.encoder_level3 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n","\n","        self.down3_4 = Downsample(int(dim*2**2)) ## From Level 3 to Level 4\n","        self.latent = nn.Sequential(*[TransformerBlock(dim=int(dim*2**3), num_heads=heads[3], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[3])])\n","\n","        self.up4_3 = Upsample(int(dim*2**2)) ## From Level 4 to Level 3\n","        self.reduce_chan_level3 = nn.Conv2d(int(dim*2**1)+192, int(dim*2**2), kernel_size=1, bias=bias)\n","        self.noise_level3 = TransformerBlock(dim=int(dim*2**2) + 512, num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type)\n","        self.reduce_noise_level3 = nn.Conv2d(int(dim*2**2)+512,int(dim*2**2),kernel_size=1,bias=bias)\n","\n","\n","        self.decoder_level3 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n","\n","\n","        self.up3_2 = Upsample(int(dim*2**2)) ## From Level 3 to Level 2\n","        self.reduce_chan_level2 = nn.Conv2d(int(dim*2**2), int(dim*2**1), kernel_size=1, bias=bias)\n","        self.noise_level2 = TransformerBlock(dim=int(dim*2**1) + 224, num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type)\n","        self.reduce_noise_level2 = nn.Conv2d(int(dim*2**1)+224,int(dim*2**2),kernel_size=1,bias=bias)\n","\n","        self.decoder_level2 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n","\n","        self.up2_1 = Upsample(int(dim*2**1))  ## From Level 2 to Level 1  (NO 1x1 conv to reduce channels)\n","\n","        self.noise_level1 = TransformerBlock(dim=int(dim*2**1)+64, num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type)\n","        self.reduce_noise_level1 = nn.Conv2d(int(dim*2**1)+64,int(dim*2**1),kernel_size=1,bias=bias)\n","\n","        self.decoder_level1 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n","\n","        self.refinement = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_refinement_blocks)])\n","\n","        self.output = nn.Conv2d(int(dim*2**1), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n","\n","    def forward(self, inp_img,noise_emb = None):\n","        inp_enc_level1 = self.patch_embed(inp_img)\n","        out_enc_level1 = self.encoder_level1(inp_enc_level1)\n","        inp_enc_level2 = self.down1_2(out_enc_level1)\n","        out_enc_level2 = self.encoder_level2(inp_enc_level2)\n","        inp_enc_level3 = self.down2_3(out_enc_level2)\n","        out_enc_level3 = self.encoder_level3(inp_enc_level3)\n","        inp_enc_level4 = self.down3_4(out_enc_level3)\n","        latent = self.latent(inp_enc_level4)\n","        if self.decoder:\n","            dec3_param = self.prompt3(latent)\n","            latent = torch.cat([latent, dec3_param], 1)\n","            latent = self.noise_level3(latent)\n","            latent = self.reduce_noise_level3(latent)\n","\n","        inp_dec_level3 = self.up4_3(latent)\n","\n","        inp_dec_level3 = torch.cat([inp_dec_level3, out_enc_level3], 1)\n","        inp_dec_level3 = self.reduce_chan_level3(inp_dec_level3)\n","\n","        out_dec_level3 = self.decoder_level3(inp_dec_level3)\n","        if self.decoder:\n","            dec2_param = self.prompt2(out_dec_level3)\n","            out_dec_level3 = torch.cat([out_dec_level3, dec2_param], 1)\n","            out_dec_level3 = self.noise_level2(out_dec_level3)\n","            out_dec_level3 = self.reduce_noise_level2(out_dec_level3)\n","\n","        inp_dec_level2 = self.up3_2(out_dec_level3)\n","        inp_dec_level2 = torch.cat([inp_dec_level2, out_enc_level2], 1)\n","        inp_dec_level2 = self.reduce_chan_level2(inp_dec_level2)\n","\n","        out_dec_level2 = self.decoder_level2(inp_dec_level2)\n","        if self.decoder:\n","            dec1_param = self.prompt1(out_dec_level2)\n","            out_dec_level2 = torch.cat([out_dec_level2, dec1_param], 1)\n","            out_dec_level2 = self.noise_level1(out_dec_level2)\n","            out_dec_level2 = self.reduce_noise_level1(out_dec_level2)\n","\n","        inp_dec_level1 = self.up2_1(out_dec_level2)\n","        inp_dec_level1 = torch.cat([inp_dec_level1, out_enc_level1], 1)\n","        out_dec_level1 = self.decoder_level1(inp_dec_level1)\n","        out_dec_level1 = self.refinement(out_dec_level1)\n","        out_dec_level1 = self.output(out_dec_level1) + inp_img\n","        return out_dec_level1\n","\n","\n","class ch_shuffle_high_text(nn.Module):\n","    def __init__(self, ch_dim,num_heads,LayerNorm_type,ffn_expansion_factor, bias,lin_ch=512):\n","        super(ch_shuffle_high_text, self).__init__()\n","        self.dim = ch_dim\n","        self.linear_layer1 = nn.Linear(lin_ch,lin_ch)\n","        self.linear_layer3 = nn.Linear(lin_ch,2*ch_dim)\n","# -----------------------------------------------------------------------\n","        self.conv1x1   = nn.Conv2d(ch_dim, 2*ch_dim, kernel_size=1, stride=1, padding=0) #\n","        self.conv_out   = nn.Conv2d(2*ch_dim, ch_dim, kernel_size=1, stride=1, padding=0) #\n","        self.norm1 = LayerNorm(ch_dim, LayerNorm_type)\n","        self.norm2 = LayerNorm(ch_dim, LayerNorm_type)\n","        self.norm3 = LayerNorm(ch_dim, LayerNorm_type)\n","        self.select_attn = Topm_CrossAttention_Restormer(ch_dim, num_heads, bias=False)\n","        self.ffn = FeedForward(ch_dim, ffn_expansion_factor, bias)\n","    def forward(self, img_featur, text_code):\n","        b,c,_,_ = img_featur.shape\n","        img_feature2 = img_featur\n","        deg_prompt = text_code\n","        text_code = self.linear_layer1(text_code)\n","        text_code = self.linear_layer3(text_code)\n","        soft_values, soft_indices = torch.topk(text_code, k=2*self.dim)\n","        img_featur = self.conv1x1(img_featur)\n","        shuffled_img = img_featur[torch.arange(b).unsqueeze(1), soft_indices, :, :] # shuffle\n","\n","        q = self.conv_out(shuffled_img)\n","        att = self.select_attn(self.norm1(q),self.norm2(img_feature2),deg_prompt)\n","        output = att + self.ffn(self.norm3(att))\n","        return output,img_feature2\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class AdvancedLearnableMaskGenerator(nn.Module):\n","    \"\"\"\n","    Generates a highly context-aware C x C mask.\n","    It uses the original feature, the shuffled feature, and the degradation prompt as inputs.\n","    \"\"\"\n","    def __init__(self, channel_dim, prompt_embedding_dim=512, hidden_dim=128):\n","        super().__init__()\n","        self.channel_dim = channel_dim\n","\n","        # This processor now takes the concatenated vectors from BOTH feature maps (original + shuffled).\n","        self.feature_processor = nn.Linear(channel_dim * 2, hidden_dim)\n","\n","        # This processor handles the degradation prompt as before.\n","        self.prompt_processor = nn.Linear(prompt_embedding_dim, hidden_dim)\n","\n","        # The final layers that generate the mask from all combined information.\n","        self.mask_generator = nn.Sequential(\n","            nn.Linear(hidden_dim * 2, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, channel_dim * channel_dim)\n","        )\n","\n","    def forward(self, original_feature, shuffled_feature, degradation_prompt):\n","        \"\"\"\n","        Args:\n","            original_feature (torch.Tensor): The original feature map from the encoder (B, C, H, W).\n","            shuffled_feature (torch.Tensor): The shuffled feature map from the DGCPM (B, C, H, W).\n","            degradation_prompt (torch.Tensor): The CLIP text embedding for the task (B, prompt_embedding_dim).\n","\n","        Returns:\n","            torch.Tensor: A learnable soft mask of shape (B, C, C).\n","        \"\"\"\n","        # 1. Condense both original and shuffled features into descriptor vectors.\n","        original_vector = F.adaptive_avg_pool2d(original_feature, 1).squeeze(-1).squeeze(-1)\n","        shuffled_vector = F.adaptive_avg_pool2d(shuffled_feature, 1).squeeze(-1).squeeze(-1)\n","\n","        # 2. Concatenate the feature vectors to capture their relationship.\n","        combined_feature_vector = torch.cat([original_vector, shuffled_vector], dim=1)\n","\n","        # 3. Process the combined features and the degradation prompt separately.\n","        processed_features = self.feature_processor(combined_feature_vector)\n","        processed_prompt = self.prompt_processor(degradation_prompt)\n","\n","        # 4. Concatenate all processed information.\n","        final_combined_info = torch.cat([processed_features, processed_prompt], dim=1)\n","\n","        # 5. Generate the final mask values.\n","        mask_values = self.mask_generator(final_combined_info)\n","\n","        # 6. Reshape and apply sigmoid to create the soft mask.\n","        mask = mask_values.view(-1, self.channel_dim, self.channel_dim)\n","        mask = torch.sigmoid(mask)\n","\n","        return mask\n","\n","class Topm_CrossAttention_Restormer(nn.Module):\n","    \"\"\"\n","    Modified CAAPM using the AdvancedLearnableMaskGenerator.\n","    The __init__ signature is compatible with your main script's call.\n","    \"\"\"\n","    def __init__(self, dim, num_heads, prompt_embedding_dim=512, bias=False):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n","\n","        # Projections for Query, Key, Value\n","        self.query_proj = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n","        self.key_proj = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n","        self.value_proj = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n","        self.output_proj = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n","\n","        # --- MODIFICATION 1: Instantiate the new, more advanced generator ---\n","        self.mask_generator = AdvancedLearnableMaskGenerator(\n","            channel_dim=dim // num_heads,\n","            prompt_embedding_dim=prompt_embedding_dim\n","        )\n","\n","    def forward(self, shuffled_feature, original_feature, degradation_prompt):\n","        b, c, h, w = original_feature.shape\n","        head_dim = c // self.num_heads\n","\n","        # The query is derived from the shuffled, task-aligned feature (Q).[1]\n","        query = self.query_proj(shuffled_feature).view(b, self.num_heads, head_dim, h * w)\n","        # The key and value are from the original, context-rich feature (F_n).[1]\n","        key = self.key_proj(original_feature).view(b, self.num_heads, head_dim, h * w)\n","        value = self.value_proj(original_feature).view(b, self.num_heads, head_dim, h * w)\n","\n","        query = F.normalize(query, dim=-1)\n","        key = F.normalize(key, dim=-1)\n","        raw_attn_scores = (query @ key.transpose(-2, -1)) * self.temperature\n","\n","        # --- MODIFICATION 2: Prepare inputs and generate the mask for each head ---\n","        original_feature_reshaped = original_feature.view(b, self.num_heads, head_dim, h, w)\n","        shuffled_feature_reshaped = shuffled_feature.view(b, self.num_heads, head_dim, h, w)\n","\n","        masks =[]\n","        for i in range(self.num_heads):\n","            # Pass the feature slice for the current head from BOTH original and shuffled maps.\n","            mask = self.mask_generator(\n","                original_feature_reshaped[:, i,...],\n","                shuffled_feature_reshaped[:, i,...],\n","                degradation_prompt\n","            )\n","            masks.append(mask.unsqueeze(1))\n","\n","        learnable_mask = torch.cat(masks, dim=1)\n","\n","        # Apply the new, more powerful mask\n","        preferential_scores = raw_attn_scores * learnable_mask\n","        attention_probs = F.softmax(preferential_scores, dim=-1)\n","\n","        output = (attention_probs @ value)\n","        output = output.view(b, c, h, w)\n","        output = self.output_proj(output)\n","        return output\n","\n","class ChannelShuffle_skip_textguaid(nn.Module):\n","    def __init__(self,\n","        inp_channels=3,\n","        out_channels=3,\n","        dim = 48,\n","        num_blocks = [4,6,6,8],\n","        num_refinement_blocks = 4,\n","        heads = [1,2,4,8],\n","        ffn_expansion_factor = 2.66,\n","        bias = False,\n","        LayerNorm_type = 'WithBias',   ## Other option 'BiasFree'\n","        device = \"cuda:1\",\n","        # decoder = False,\n","        dual_pixel_task = False        ## True for dual-pixel defocus deblurring only. Also set inp_channels=6\n","    ):\n","        super(ChannelShuffle_skip_textguaid, self).__init__()\n","        self.device = device\n","\n","        self.patch_embed = OverlapPatchEmbed(inp_channels, dim)\n","        self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n","        self.encoder_shuffle_channel1 = ch_shuffle_high_text(ch_dim = dim,num_heads=heads[0],LayerNorm_type=LayerNorm_type,ffn_expansion_factor=ffn_expansion_factor,bias=bias) # encoder level1 shuffle\n","\n","        self.down1_2 = Downsample(dim) ## From Level 1 to Level 2\n","        self.encoder_level2 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n","        self.encoder_shuffle_channel2 = ch_shuffle_high_text(ch_dim = int(dim*2**1),num_heads=heads[1],LayerNorm_type=LayerNorm_type,ffn_expansion_factor=ffn_expansion_factor,bias=bias) # encoder level2 shuffle\n","\n","        self.down2_3 = Downsample(int(dim*2**1)) ## From Level 2 to Level 3\n","        self.encoder_level3 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n","        self.encoder_shuffle_channel3 = ch_shuffle_high_text(ch_dim = int(dim*2**2),num_heads=heads[2],LayerNorm_type=LayerNorm_type,ffn_expansion_factor=ffn_expansion_factor,bias=bias) # encoder level3 shuffle\n","\n","        self.down3_4 = Downsample(int(dim*2**2)) ## From Level 3 to Level 4\n","        self.latent = nn.Sequential(*[TransformerBlock(dim=int(dim*2**3), num_heads=heads[3], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[3])])\n","        self.latent_shuffle_channel = ch_shuffle_high_text(ch_dim = int(dim*2**3),num_heads=heads[3],LayerNorm_type=LayerNorm_type,ffn_expansion_factor=ffn_expansion_factor,bias=bias) # latent latent shuffle\n","\n","        self.up4_3 = Upsample(int(dim*2**3)) ## From Level 4 to Level 3\n","        self.reduce_chan_level3 = nn.Conv2d(int(dim*2**3), int(dim*2**2), kernel_size=1, bias=bias)\n","        self.decoder_level3 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n","\n","        self.up3_2 = Upsample(int(dim*2**2)) ## From Level 3 to Level 2\n","        self.reduce_chan_level2 = nn.Conv2d(int(dim*2**2), int(dim*2**1), kernel_size=1, bias=bias)\n","        self.decoder_level2 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n","\n","        self.up2_1 = Upsample(int(dim*2**1))  ## From Level 2 to Level 1  (NO 1x1 conv to reduce channels)\n","        self.decoder_level1 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n","\n","        self.refinement = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_refinement_blocks)])\n","        #### For Dual-Pixel Defocus Deblurring Task ####\n","        # self.dual_pixel_task = dual_pixel_task\n","        # if self.dual_pixel_task:\n","        #     self.skip_conv = nn.Conv2d(dim, int(dim*2**1), kernel_size=1, bias=bias)\n","        # ###########################\n","\n","        self.output = nn.Conv2d(int(dim*2**1), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n","\n","    def forward(self, inp_img ,text_code): # ,text_code\n","        # text_code = torch.randn(1,512).to(self.device) # 这个在测试模型参数量和计算量时候加上\n","        inp_enc_level1 = self.patch_embed(inp_img) # ch 3--\u003edim:48\n","        out_enc_level1 = self.encoder_level1(inp_enc_level1) # ch dim:48--\u003edim:48\n","\n","        inp_enc_level2 = self.down1_2(out_enc_level1) # ch dim:48--\u003edim*2:96\n","        out_enc_level2 = self.encoder_level2(inp_enc_level2) # ch dim*2:96--\u003edim*2:96\n","\n","        inp_enc_level3 = self.down2_3(out_enc_level2) # ch dim*2:96--\u003edim*2*2:192\n","        out_enc_level3 = self.encoder_level3(inp_enc_level3) # ch dim*2*2:192--\u003edim*2*2:192\n","\n","        inp_enc_level4 = self.down3_4(out_enc_level3)\n","        latent = self.latent(inp_enc_level4)\n","        latent,_ = self.latent_shuffle_channel(latent,text_code) # latent latent shuffle\n","\n","        inp_dec_level3 = self.up4_3(latent)\n","        outt1,_ = self.encoder_shuffle_channel3(out_enc_level3,text_code)\n","        inp_dec_level3 = torch.cat([inp_dec_level3, outt1], 1)\n","        inp_dec_level3 = self.reduce_chan_level3(inp_dec_level3)\n","        out_dec_level3 = self.decoder_level3(inp_dec_level3)\n","\n","        inp_dec_level2 = self.up3_2(out_dec_level3)\n","        outt2,_ = self.encoder_shuffle_channel2(out_enc_level2,text_code)\n","        inp_dec_level2 = torch.cat([inp_dec_level2, outt2], 1)\n","        inp_dec_level2 = self.reduce_chan_level2(inp_dec_level2)\n","        out_dec_level2 = self.decoder_level2(inp_dec_level2)\n","\n","        inp_dec_level1 = self.up2_1(out_dec_level2)\n","        outt3,_ = self.encoder_shuffle_channel1(out_enc_level1,text_code)\n","        inp_dec_level1 = torch.cat([inp_dec_level1, outt3], 1)\n","        out_dec_level1 = self.decoder_level1(inp_dec_level1)\n","\n","        out_dec_level1 = self.refinement(out_dec_level1)\n","\n","        #### For Dual-Pixel Defocus Deblurring Task ####\n","        # if self.dual_pixel_task:\n","        #     out_dec_level1 = out_dec_level1 + self.skip_conv(inp_enc_level1)\n","        #     out_dec_level1 = self.output(out_dec_level1)\n","        # ###########################\n","        # else:\n","        out_dec_level1 = self.output(out_dec_level1) + inp_img\n","\n","        return out_dec_level1\n","\n","\n","\n","##---------- Restormer -----------------------\n","class Restormer(nn.Module):\n","    def __init__(self,\n","        inp_channels=3,\n","        out_channels=3,\n","        dim = 48,\n","        num_blocks = [4,6,6,8],\n","        num_refinement_blocks = 4,\n","        heads = [1,2,4,8],\n","        ffn_expansion_factor = 2.66,\n","        bias = False,\n","        LayerNorm_type = 'WithBias',   ## Other option 'BiasFree'\n","        dual_pixel_task = False        ## True for dual-pixel defocus deblurring only. Also set inp_channels=6\n","    ):\n","\n","        super(Restormer, self).__init__()\n","\n","        self.patch_embed = OverlapPatchEmbed(inp_channels, dim)\n","\n","        self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n","\n","        self.down1_2 = Downsample(dim) ## From Level 1 to Level 2\n","        self.encoder_level2 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n","\n","        self.down2_3 = Downsample(int(dim*2**1)) ## From Level 2 to Level 3\n","        self.encoder_level3 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n","\n","        self.down3_4 = Downsample(int(dim*2**2)) ## From Level 3 to Level 4\n","        self.latent = nn.Sequential(*[TransformerBlock(dim=int(dim*2**3), num_heads=heads[3], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[3])])\n","\n","        self.up4_3 = Upsample(int(dim*2**3)) ## From Level 4 to Level 3\n","        self.reduce_chan_level3 = nn.Conv2d(int(dim*2**3), int(dim*2**2), kernel_size=1, bias=bias)\n","        self.decoder_level3 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n","\n","\n","        self.up3_2 = Upsample(int(dim*2**2)) ## From Level 3 to Level 2\n","        self.reduce_chan_level2 = nn.Conv2d(int(dim*2**2), int(dim*2**1), kernel_size=1, bias=bias)\n","        self.decoder_level2 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n","\n","        self.up2_1 = Upsample(int(dim*2**1))  ## From Level 2 to Level 1  (NO 1x1 conv to reduce channels)\n","\n","        self.decoder_level1 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n","\n","        self.refinement = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_refinement_blocks)])\n","\n","        #### For Dual-Pixel Defocus Deblurring Task ####\n","        self.dual_pixel_task = dual_pixel_task\n","        if self.dual_pixel_task:\n","            self.skip_conv = nn.Conv2d(dim, int(dim*2**1), kernel_size=1, bias=bias)\n","        ###########################\n","\n","        self.output = nn.Conv2d(int(dim*2**1), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n","\n","    def forward(self, inp_img):\n","\n","        inp_enc_level1 = self.patch_embed(inp_img)\n","        out_enc_level1 = self.encoder_level1(inp_enc_level1)\n","\n","        inp_enc_level2 = self.down1_2(out_enc_level1)\n","        out_enc_level2 = self.encoder_level2(inp_enc_level2)\n","\n","        inp_enc_level3 = self.down2_3(out_enc_level2)\n","        out_enc_level3 = self.encoder_level3(inp_enc_level3)\n","\n","        inp_enc_level4 = self.down3_4(out_enc_level3)\n","        latent = self.latent(inp_enc_level4)\n","\n","        inp_dec_level3 = self.up4_3(latent)\n","        inp_dec_level3 = torch.cat([inp_dec_level3, out_enc_level3], 1)\n","        inp_dec_level3 = self.reduce_chan_level3(inp_dec_level3)\n","        out_dec_level3 = self.decoder_level3(inp_dec_level3)\n","\n","        inp_dec_level2 = self.up3_2(out_dec_level3)\n","        inp_dec_level2 = torch.cat([inp_dec_level2, out_enc_level2], 1)\n","        inp_dec_level2 = self.reduce_chan_level2(inp_dec_level2)\n","        out_dec_level2 = self.decoder_level2(inp_dec_level2)\n","\n","        inp_dec_level1 = self.up2_1(out_dec_level2)\n","        inp_dec_level1 = torch.cat([inp_dec_level1, out_enc_level1], 1)\n","        out_dec_level1 = self.decoder_level1(inp_dec_level1)\n","\n","        out_dec_level1 = self.refinement(out_dec_level1)\n","\n","        #### For Dual-Pixel Defocus Deblurring Task ####\n","        if self.dual_pixel_task:\n","            out_dec_level1 = out_dec_level1 + self.skip_conv(inp_enc_level1)\n","            out_dec_level1 = self.output(out_dec_level1)\n","        ###########################\n","        else:\n","            out_dec_level1 = self.output(out_dec_level1) + inp_img\n","\n","\n","        return out_dec_level1\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XZup8Baa_3m2"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"DYnsl8r2r9Ov"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-09-30 20:30:23.607881: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1759264223.630191    4893 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1759264223.637001    4893 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1759264223.654394    4893 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1759264223.654421    4893 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1759264223.654426    4893 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1759264223.654430    4893 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-09-30 20:30:23.659499: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","=\u003e no model found at './'\n","['denoise_15', 'denoise_25', 'denoise_50', 'derain', 'dehaze']\n","Total Denoise Ids : 5144\n","Total Rainy Ids : 24000\n","Total Hazy Ids : 72135\n","142431\n","/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","load dataset ok\n","Start bsd68/ testing Sigma=15...\n","100% 68/68 [01:29\u003c00:00,  1.32s/it]\n","Denoise sigma=15: psnr: 21.33, ssim: 0.4500\n","bsd68/test ok psnr_g15:21.3341 ssim_g15:0.4500,\n","Start bsd68/ testing Sigma=25...\n","100% 68/68 [00:48\u003c00:00,  1.40it/s]\n","Denoise sigma=25: psnr: 18.93, ssim: 0.3548\n","bsd68/test ok psnr_g25:18.9346 ssim_g25:0.3548,\n","Start bsd68/ testing Sigma=50...\n","100% 68/68 [00:51\u003c00:00,  1.32it/s]\n","Denoise sigma=50: psnr: 14.61, ssim: 0.2058\n","bsd68/test ok psnr_g50:14.6074 ssim_g50:0.2058,\n","Start testing Rain100L rain streak removal...\n","100% 100/100 [02:25\u003c00:00,  1.46s/it]\n","PSNR: 21.13, SSIM: 0.4751\n","Rain100L test ok psnr_rain:21.1332 ssim_rain:0.4751,\n","Start testing SOTS...\n","100% 500/500 [15:21\u003c00:00,  1.84s/it]\n","PSNR: 15.09, SSIM: 0.3544\n","dehaze test ok psnr_haze:15.0896 ssim_haze:0.3544,\n","test ok! psnr_noise:21.3341-0.4500_18.9346-0.3548_14.6074-0.2058,--psnr_rain:21.1332 ssim_rain:0.4751,--psnr_haze:15.0896 ssim_haze:0.3544avr18.2198-0.3680,\n","  0% 0/71215 [00:00\u003c?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/DFPIR_project/DFPIR-main/train_3D_DFPIR.py\", line 303, in \u003cmodule\u003e\n","    loss = train(train_loader, model, optimizer, epoch, args.epochs + 1,criterionL1)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/DFPIR_project/DFPIR-main/train_3D_DFPIR.py\", line 114, in train\n","    for i, ([clean_name, de_id], degrad_patch, clean_patch) in enumerate(loop_train):\n","                                                               ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/tqdm/std.py\", line 1181, in __iter__\n","    for obj in iterable:\n","               ^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 734, in __next__\n","    data = self._next_data()\n","           ^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1516, in _next_data\n","    return self._process_data(data, worker_id)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1551, in _process_data\n","    data.reraise()\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/_utils.py\", line 769, in reraise\n","    raise exception\n","OSError: Caught OSError in DataLoader worker process 2.\n","Original Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n","    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n","           ^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n","    data = [self.dataset[idx] for idx in possibly_batched_index]\n","            ~~~~~~~~~~~~^^^^^\n","  File \"/content/drive/MyDrive/DFPIR_project/DFPIR-main/utils/dataset_utils.py\", line 166, in __getitem__\n","    degrad_img = crop_img(np.array(Image.open(sample[\"clean_id\"]).convert('RGB')), base=16)\n","                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/PIL/Image.py\", line 3513, in open\n","    fp = builtins.open(filename, \"rb\")\n","         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","OSError: [Errno 5] Input/output error: './train_data/haze/synthetic/0227_0.8_0.16.jpg'\n","\n"]}],"source":["!python train_3D_DFPIR.py \\\n","    --epochs 1 \\\n","    --gpu \"0\" \\\n","    --cuda 0 \\\n","    --batch_size 2 \\\n","    --save_item 20000\\\n","    --save_dir \"./checkpoints/\" \\\n","    --derain_dir \"./train_data/rain/\" \\\n","    --dehaze_dir \"./train_data/haze/\" \\\n","    --denoise_dir \"./train_data/noise/\" \\\n","    --derain_path \"./test/derain/\" \\\n","    --dehaze_path \"./test/dehaze/\" \\\n","    --denoise_path \"./test/denoise/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uzTjVjP0r9Ov","outputId":"1b766681-1e87-4071-e985-5f6905ddaa44"},"outputs":[{"name":"stdout","output_type":"stream","text":["=\u003e loading model './DFPIR-3D_p_n34.14-0.9354_31.47-0.8928_28.25-0.8059_p_r38.65-0.9821_p_h31.87-0.9800avr32.88-0.9192.pth.tar'\n","Strart test 3D\n","Start bsd68/ testing Sigma=15...\n","100%|\u001b[35m███████████████████████████████████████████\u001b[0m| 68/68 [00:39\u003c00:00,  1.71it/s]\u001b[0m\n","Denoise sigma=15: psnr: 34.15, ssim: 0.9357\n","Average Inference Time: 39795.82 ms\n","bsd68/test ok psnr_g15:34.1456 ssim_g15:0.9357,\n","Start bsd68/ testing Sigma=25...\n"," 13%|\u001b[35m█████▊                                      \u001b[0m| 9/68 [00:05\u003c00:34,  1.69it/s]\u001b[0m^C\n"," 13%|\u001b[35m█████▊                                      \u001b[0m| 9/68 [00:05\u003c00:38,  1.53it/s]\u001b[0m\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\", line 643, in _save\n","    fh = fp.fileno()\n","         ^^^^^^^^^\n","AttributeError: '_idat' object has no attribute 'fileno'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/kaggle/working/DFPIR-main/test_3D_DFPIR.py\", line 177, in \u003cmodule\u003e\n","    psnr_n15,ssim_n15,psnr_n25,ssim_n25,psnr_n50,ssim_n50,psnr_rain,ssim_rain,psnr_haze,ssim_haze =test( model, criterionL1) \n","                                                                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/kaggle/working/DFPIR-main/test_3D_DFPIR.py\", line 74, in test\n","    psnr_g25,ssim_g25 = test_Denoise(model, testset, sigma=25,text_prompt=inputext[1])\n","                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/kaggle/working/DFPIR-main/test_3D_DFPIR.py\", line 116, in test_Denoise\n","    save_image_tensor(restored, output_path + clean_name[0] + filename + '.png')\n","  File \"/kaggle/working/DFPIR-main/utils/image_io.py\", line 161, in save_image_tensor\n","    p.save(output_path)\n","  File \"/usr/local/lib/python3.11/dist-packages/PIL/Image.py\", line 2581, in save\n","    save_handler(self, fp, filename)\n","  File \"/usr/local/lib/python3.11/dist-packages/PIL/PngImagePlugin.py\", line 1492, in _save\n","    ImageFile._save(\n","  File \"/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\", line 647, in _save\n","    _encode_tile(im, fp, tile, bufsize, None, exc)\n","  File \"/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\", line 673, in _encode_tile\n","    errcode, data = encoder.encode(bufsize)[1:]\n","                    ^^^^^^^^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n"]}],"source":["!python test_3D_DFPIR.py \\\n","    --gpu \"0\" \\\n","    --cuda 0 \\\n","    --pretrained_1 \"./DFPIR-3D_p_n34.14-0.9354_31.47-0.8928_28.25-0.8059_p_r38.65-0.9821_p_h31.87-0.9800avr32.88-0.9192.pth.tar\" \\\n","    --denoise_path \"test/denoise/\" \\\n","    --derain_path \"test/derain/\" \\\n","    --dehaze_path \"test/dehaze/\" \\\n","    --output_path \"output/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u4_nf_9zr9Ow","outputId":"1fb33fc3-0382-4d5b-b776-de235b038a5a"},"outputs":[{"data":{"text/plain":"'/kaggle/working/DFPIR-main'"},"execution_count":108,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GQDx0AHjr9Ow"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","version":""},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":8366223,"sourceId":13200893,"sourceType":"datasetVersion"},{"datasetId":8366578,"sourceId":13201457,"sourceType":"datasetVersion"}],"dockerImageVersionId":31090,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"}},"nbformat":4,"nbformat_minor":0}